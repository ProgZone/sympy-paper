The \texttt{Float} class represents an arbitrary-precision binary floating-point
by storing it value and precision (in bits). An operation between two \texttt{Float}
inputs is rounded to the larger of the two precisions.
Since Python floating-point literals automatically evaluate to \texttt{double}
(53-bit) precision, strings should be used to input precise decimal values:

\begin{verbatim}
>>> Float(1.1)
1.10000000000000
>>> Float(1.1, 30)   # precision equivalent to 30 digits
1.10000000000000008881784197001
>>> Float("1.1", 30)
1.10000000000000000000000000000
\end{verbatim}

The preferred way to evaluate an expression numerically is with the
\texttt{evalf} method, which internally estimates the number of accurate
bits of the floating-point
approximation for each sub-expression, and adaptively increases the
working precision until the estimated accuracy of the
final result matches the sought number of decimal digits.

The internal error tracking does not provide rigorous error bounds
(in the sense of interval arithmetic) and cannot be used to accurately track
uncertainty in measurement data;
the sole purpose is to mitigate loss of accuracy that typically occurs
when converting symbolic expressions to numerical values. A well-known,
notorious case
is that of catastrophic cancellation. This is illustrated by the following
example (the input 25 specifies that 25 digits are sought):

\begin{verbatim}
>>> cos(exp(-100)).evalf(25) - 1
0
>>> (cos(exp(-100)) - 1).evalf(25)
-6.919482633683687653243407e-88
\end{verbatim}

The \texttt{evalf} method works with complex numbers and supports
more complicated expressions, such as
special functions, infinite series and integrals.

SymPy does not track the accuracy of
approximate numbers outside of \texttt{evalf}.
The familiar dangers of floating-point arithmetic apply~\cite{goldberg1991every},
and symbolic expressions containing floating-point numbers should be treated
with reasonable caution.
This approach is similar to Maple and Maxima.
By contrast, Mathematica uses a form
of significance arithmetic~\cite{Sofroniou2005precise} for approximate numbers.
This offers further protection against numerical errors,
but leads to non-obvious semantics while
still not being mathematically rigorous
(for a critique of significance arithmetic, see Fateman~\cite{Fateman1992}).

\subsection{The mpmath library}

The implementation of arbitrary-precision floating-point arithmetic is
supplied by the mpmath library, which originally was developed as a SymPy
module but subsequently has been moved to a standalone pure Python package.
The basic datatypes in mpmath are \texttt{mpf} and \texttt{mpc}, which
respectively act as multiprecision substitutes for Python's \texttt{float} and
\texttt{complex}. The floating-point precision is controlled by a global
context:

% doctest printer doesn't display "mpf"
% no-doctest
\begin{verbatim}
>>> import mpmath
>>> mpmath.mp.dps = 30    # 30 digits of precision
>>> mpmath.mpf("0.1") + mpmath.exp(-50)
mpf('0.100000000000000000000192874984794')
>>> print(_)   # pretty-printed
0.100000000000000000000192874985
\end{verbatim}

For pure numerical computing, it is convenient to use mpmath directly
with \texttt{from mpmath import *} (it is best to avoid such an
import statement when using SymPy simultaneously, since numerical
functions such as \texttt{exp} will collide names with the symbolic counterparts
in SymPy).

Like SymPy, mpmath is a pure Python library.
Internally, mpmath represents a floating-point number
${(-1)}^s x \cdot 2^y$ by a tuple $(s, x, y, b)$ where
$x$ and $y$ are arbitrary-size Python integers
and the redundant integer $b$ stores the bit length of $x$ for quick access.
If GMPY~\cite{GMPY} is installed, mpmath automatically uses
the \texttt{gmpy.mpz} type for~$x$, and GMPY methods
for rounding-related operations, improving performance.

The mpmath library supports
special functions, root-finding, linear algebra, polynomial approximation,
and numerical computation of limits, derivatives, integrals, infinite
series, and ODE solutions. All features work in arbitrary precision
and use algorithms that allow computing hundreds of digits rapidly,
except in degenerate cases.

The double exponential (tanh-sinh) quadrature is used for numerical
integration by default. For smooth integrands, this algorithm usually
converges extremely rapidly, even when the integration interval is infinite
or singularities are present at the endpoints~\cite{takahasi1974double,bailey2005comparison}.
However, for good performance, singularities
in the middle of the interval must be specified
by the user.
To evaluate slowly converging limits and infinite series, mpmath
automatically tries Richardson extrapolation and the
Shanks transformation
(Euler-Maclaurin summation can also be used)~\cite{BenderOrszag1999}.
A function to evaluate oscillatory integrals by means of convergence
acceleration is also available.

A wide array of higher mathematical functions are implemented
with full support for complex values of all parameters and arguments,
including complete and incomplete gamma functions,
Bessel functions, orthogonal polynomials, elliptic functions and integrals,
zeta and polylogarithm functions,
the generalized hypergeometric function, and the Meijer G-function.
The Meijer G-function instance
$G_{1, 3}^{3, 0}\left(0 ; \tfrac{1}{2}, -1, - \tfrac{3}{2} | x \right)$
is a good test case~\cite{Toth2007}; past versions of both Maple and
Mathematica produced incorrect numerical values for large $x > 0$.
Here, mpmath automatically removes an internal singularity
and compensates for cancellations (amounting to 656 bits
of precision when $x = 10000$), giving correct values:
\begin{verbatim}
>>> mpmath.mp.dps = 15
>>> mpmath.meijerg([[],[0]],[[-0.5,-1,-1.5],[]],10000)
2.4392576907199564e-94
\end{verbatim}

Equivalently, with SymPy's interface this function can be evaluated as:
\begin{verbatim}
>>> meijerg([[],[0]],[[-S(1)/2,-1,-S(3)/2],[]],10000).evalf()
2.43925769071996e-94
\end{verbatim}

Symbolic integration and summation often produces hypergeometric
and Meijer G-function closed forms (see section~\ref{sec:calculus});
numerical evaluation of such special functions is a useful complement
to direct numerical integration and summation.

